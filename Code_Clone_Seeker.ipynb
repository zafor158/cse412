{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zafor158/cse412/blob/main/Code_Clone_Seeker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqrWht_1fa-_"
      },
      "source": [
        "**Step 1: Install Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u12W42YyfeSm",
        "outputId": "fbe30dd6-0b69-4aa1-e797-aa9327ae6e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or-BundAfl_O"
      },
      "source": [
        "**Step 2: Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq57wd1qfous"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgQhZfiVfvnc"
      },
      "source": [
        "**Step 3: Read JSONL File into a DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTOfgeMNfxYg"
      },
      "outputs": [],
      "source": [
        "# Replace 'our_dataset.jsonl' with the actual file path\n",
        "file_path = '/content/data.jsonl'\n",
        "\n",
        "# Read the JSONL file into a DataFrame\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        json_line = json.loads(line)\n",
        "        data.append(json_line)\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhxABQEf914"
      },
      "source": [
        "**Step 4: Explore and Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgCu3XlOf_fZ",
        "outputId": "04a2e7c1-6fc5-4d32-eb71-27e16f7a7d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                func       idx\n",
            "0      public static void main(String[] args) {\\n...  10000832\n",
            "1      public synchronized String getSerialNumber...  10005623\n",
            "2              public Object run() {\\n           ...  10005624\n",
            "3      public String post() {\\n        if (conten...  10005674\n",
            "4      @Override\\n    public void onCreate(Bundle...  10005879\n"
          ]
        }
      ],
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVC1LrnlkdWC",
        "outputId": "2619227b-db43-445c-e953-37e10d0e4cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                func  \\\n",
            "0      public static void main(String[] args) {\\n...   \n",
            "1      public synchronized String getSerialNumber...   \n",
            "2              public Object run() {\\n           ...   \n",
            "3      public String post() {\\n        if (conten...   \n",
            "4      @Override\\n    public void onCreate(Bundle...   \n",
            "\n",
            "                                        cleaned_code  \n",
            "0  public static void main string args int string...  \n",
            "1  public synchronized string getserialnumber ser...  \n",
            "2  public object run try messagedigest digest sha...  \n",
            "3  public string post content null return type so...  \n",
            "4  override public void oncreate bundle savedinst...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_dataset.jsonl' with the actual file path\n",
        "file_path = '/content/data.jsonl'\n",
        "data = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        json_line = json.loads(line)\n",
        "        data.append(json_line)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tokenization, Lowercasing, Stopword Removal, and Lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_code(code):\n",
        "    # Tokenize the code\n",
        "    tokens = word_tokenize(code)\n",
        "\n",
        "    # Lowercase, remove stopwords, and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'func' field\n",
        "df['cleaned_code'] = df['func'].apply(lambda x: preprocess_code(x))\n",
        "\n",
        "# Display the final DataFrame with cleaned code snippets\n",
        "print(df[['func', 'cleaned_code']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amic55KMr3Do"
      },
      "source": [
        "**Step 5: Implement Machine Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3fifuMWr5ky"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Assuming 'cleaned_code' is the key for preprocessed code snippets\n",
        "code_snippets = df['cleaned_code']\n",
        "\n",
        "# Tokenize and vectorize code snippets using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(code_snippets)\n",
        "\n",
        "# Use linear kernel for similarity computation\n",
        "similarity_matrix = linear_kernel(tfidf_matrix , tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thzeY8f3EGAO"
      },
      "source": [
        "**Streamlite App**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5T_hUrSIFEp",
        "outputId": "51eef8f7-0f41-4a7e-8c37-e5c1a2ba8835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the DataFrame\n",
        "file_path = '/content/data.jsonl'  # Replace with the actual file path\n",
        "data = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        json_line = json.loads(line)\n",
        "        data.append(json_line)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tokenization, Lowercasing, Stopword Removal, and Lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_code(code):\n",
        "    tokens = word_tokenize(code)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'func' field and create a new 'cleaned_code' column\n",
        "df['cleaned_code'] = df['func'].apply(lambda x: preprocess_code(x))\n",
        "\n",
        "# Tokenize and vectorize code snippets using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_code'])\n",
        "\n",
        "# Use linear kernel for similarity computation\n",
        "similarity_matrix = linear_kernel(tfidf_matrix , tfidf_matrix)\n",
        "\n",
        "# Streamlit App\n",
        "def main():\n",
        "    st.title(\"Duplicate Code Detection App\")\n",
        "\n",
        "    # User input\n",
        "    user_code = st.text_area(\"Enter your code snippet here:\")\n",
        "\n",
        "    if st.button(\"Detect Similarity\"):\n",
        "        # Calculate similarity with all code snippets in the dataset\n",
        "        similarity_percentages = [calculate_similarity(user_code, ref_code) for ref_code in df['cleaned_code']]\n",
        "\n",
        "        # Display the maximum similarity percentage\n",
        "        max_similarity_percentage = max(similarity_percentages)\n",
        "        st.text(f\"Maximum Similarity Percentage: {max_similarity_percentage:.2f}%\")\n",
        "\n",
        "def calculate_similarity(user_code, reference_code):\n",
        "    # Tokenize and vectorize user code\n",
        "    user_vector = tfidf_vectorizer.transform([user_code])\n",
        "\n",
        "    # Tokenize and vectorize reference code\n",
        "    reference_vector = tfidf_vectorizer.transform([reference_code])\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = cosine_similarity(user_vector, reference_vector).flatten()[0]\n",
        "\n",
        "    # Convert similarity to percentage\n",
        "    #similarity_percentage = similarity * 100\n",
        "    # Convert similarity to percentage (scaling to 0-100)\n",
        "    similarity_percentage = (similarity + 1) * 50\n",
        "\n",
        "    return similarity_percentage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAmDuuJ8uujP",
        "outputId": "9607e80b-2701-4d9e-8b24-587bd3f82747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vELOx97fuzdJ",
        "outputId": "d254fe72-f9a2-4a08-8f41-f184f86da808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.106.103.88\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgHB-ymou3Ag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594b34ed-b4d8-437b-95ba-fe1f6d93989f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[..................] / rollbackFailedOptional: verb npm-session dfb7704cf2e71df\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.103.88:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 6.839s\n",
            "your url is: https://eager-otters-relate.loca.lt\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}